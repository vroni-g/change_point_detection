ts_reshape(type='long') %>%
group_by(year) %>%
summarise(annual_mean = mean(value))
m <- ts(means$annual_mean,frequency = 1, start(1800))
return(m)
}
n <- 10000
# strucchange package ----
#*********************************************************************
library(strucchange)
# Generalized fluctuation tests
#************
test_CPD_efp <- function(n, fun, trend, h = NULL){
p <- c()
for (i in 1:n) {
#d <- ts(rnorm(12*20,0,.2)+trend, frequency=12, start=c(2000,1))
d <- annual_means(trend)
#d <- ts(rnorm(12*20,0,.2), frequency=12, start=c(2000,1))
temp <- efp(d ~ 1, d, type = fun, h = h, rescale = TRUE)
res <-  sctest(temp)
p <- c(p, res$p.value)
}
fpr <- as.double(sum(p<0.05))/as.double(n)
cat(paste('False positive rate for efp function ', fun,' is: ', fpr))
return(p)
}
n <- 10000
# residual based:
rec_cusum <- test_CPD_efp(n, 'Rec-CUSUM', trend) # annual mean: 0.0404, 0.0457; random data (no season): 0.0472, 0.0426
# Generalized fluctuation tests
#************
test_CPD_efp <- function(n, fun, h = NULL){
p <- c()
for (i in 1:n) {
#d <- ts(rnorm(12*20,0,.2)+trend, frequency=12, start=c(2000,1))
d <- annual_means(trend)
#d <- ts(rnorm(12*20,0,.2), frequency=12, start=c(2000,1))
temp <- efp(d ~ 1, d, type = fun, h = h, rescale = TRUE)
res <-  sctest(temp)
p <- c(p, res$p.value)
}
fpr <- as.double(sum(p<0.05))/as.double(n)
cat(paste('False positive rate for efp function ', fun,' is: ', fpr))
return(p)
}
# residual based:
rec_cusum <- test_CPD_efp(n, 'Rec-CUSUM') # annual mean: 0.0404, 0.0457; random data (no season): 0.0472, 0.0426
#trend <- rep(sin(2*pi*c(0:11)/12),20)
trend <- rep(sin(2*pi*c(0:11)/12),200) # trend for annual means
# Generalized fluctuation tests
#************
test_CPD_efp <- function(n, fun, trend, h = NULL){
p <- c()
for (i in 1:n) {
#d <- ts(rnorm(12*20,0,.2)+trend, frequency=12, start=c(2000,1))
d <- annual_means(trend)
#d <- ts(rnorm(12*20,0,.2), frequency=12, start=c(2000,1))
temp <- efp(d ~ 1, d, type = fun, h = h, rescale = TRUE)
res <-  sctest(temp)
p <- c(p, res$p.value)
}
fpr <- as.double(sum(p<0.05))/as.double(n)
cat(paste('False positive rate for efp function ', fun,' is: ', fpr))
return(p)
}
n <- 10000
# residual based:
rec_cusum <- test_CPD_efp(n, 'Rec-CUSUM', trend) # annual mean: 0.0404, 0.0457; random data (no season): 0.0472, 0.0426
library(TSstudio)
# residual based:
rec_cusum <- test_CPD_efp(n, 'Rec-CUSUM', trend) # annual mean: 0.0404, 0.0457; random data (no season): 0.0472, 0.0426
# warum geht das so schnell???? Da ist doch was faul oder dauert erstellen der time series bei den andern so lange?
for (c in coeffs){
p <- c()
n <- 10000
for (i in n){
d <- arima.sim(model = list(order = c(1, 0, 0), ar = c), n = 250)
res <-  efp(d ~ 1, d, type = 'Rec-CUSUM') %>%
sctest()
p <- c(p, res$p.value)
}
fpr <- sum(p<0.05)/n
cat(paste('False positive rate for Rec-CUSUM with autocorrelation ', c,' is: ', fpr, '\n'))
fprs[i] <- c(c, fpr)
i <- i + 1
}
# check false positive rate of CUSUM method applied to autocorrelated noise data
#***********************
library(strucchange)
library(tidyverse)
coeffs <- seq(0, 0.9, by=0.1)
fprs <- vector("list", 11)
n <- 10000
i <- 1
c <- 0.2
d <- arima.sim(model = list(order = c(1, 0, 0), ar = c), n = 200)
plot(d)
n <-  10000
p <- c()
for (i in n){
d <- arima.sim(model = list(order = c(1, 0, 0), ar = c), n = 200)
res <-  efp(d ~ 1, d, type = 'Rec-CUSUM') %>%
sctest()
p <- c(p, res$p.value)
}
fpr <- sum(p<0.05)/n
cat(paste('False positive rate for Rec-CUSUM with autocorrelation ', c,' is: ', fpr, '/n'))
n <-  10000
p <- c()
for (i in n){
#d <- arima.sim(model = list(order = c(1, 0, 0), ar = c), n = 200)
d <- ts(rnorm(12*20,0,.2), frequency=12, start=c(2000,1))
res <-  efp(d ~ 1, d, type = 'Rec-CUSUM') %>%
sctest()
p <- c(p, res$p.value)
}
# check false positive rate of CUSUM method applied to autocorrelated noise data
#***********************
library(strucchange)
library(tidyverse)
coeffs <- seq(0, 0.9, by=0.1)
fprs <- vector("list", 11)
n <- 10000
i <- 1
n <- 5000
for (c in coeffs){
p <- c()
n <- 10000
for (i in 1:n){
d <- arima.sim(model = list(order = c(1, 0, 0), ar = c), n = 250)
res <-  efp(d ~ 1, d, type = 'Rec-CUSUM') %>%
sctest()
p <- c(p, res$p.value)
}
fpr <- sum(p<0.05)/n
cat(paste('False positive rate for Rec-CUSUM with autocorrelation ', c,' is: ', fpr, '\n'))
fprs[i] <- c(c, fpr)
i <- i + 1
}
warnings()
n <- 10000
for (i in 1:n){
d <- arima.sim(model = list(order = c(1, 0, 0), ar = 1), n = 250)
res <-  efp(d ~ 1, d, type = 'Rec-CUSUM') %>%
sctest()
p <- c(p, res$p.value)
}
# SCRIPT FOR TESTING FUNCTIONS
library("devtools")
load_all()
# ADDITIONAL NEEDED LIBRARIES
library(tidyverse) # for pipe: %>%
library(fields) # for nicer basic plotting: image.plot
# test results for nperm=1000, detrended data
# saveRDS(perm_results, file = "testing/detrended_temp_data_nperm_1000.rds")
perm_results<- readRDS("testing/detrended_temp_data_nperm_1000.rds")
str(perm_results)
View(perm_results)
perm_results$stcs_maxT
perm_results$stcs_maxT[!is.finite(perm_results$stcs_maxT)] <- 0
# bootstrap check to check false positive rate
sim_length<- 1000
bootstrap_sample<- 100
fpr_length<- 100
alpha<- 0.05
fpr_sim_stcs<- vector(length = sim_length)
fpr_sim_maxt<- vector(length = sim_length)
fpr_sim_bivariate<- vector(length = sim_length)
for(j in 1:sim_length){
fpr_stcs<- vector(length = fpr_length)
fpr_maxt<- vector(length = fpr_length)
fpr_bivariate<- vector(length = fpr_length)
for (i in 1:fpr_length){
ind<- sample(x = length(perm_results$maxT), size = bootstrap_sample, replace = TRUE)
tmp_stcs<- perm_results$stcs[ind]
tmp_maxt<- perm_results$maxT[ind]
tmp_stcs_maxt<- perm_results$stcs_maxT[ind]
tmp_stcs_maxt[!is.finite(tmp_stcs_maxt)]<- 0
# gets the threshold for the current sample
q_thr_stcs<- quantile(tmp_stcs, probs = 1-alpha, names = FALSE)
q_thr_maxt<- quantile(tmp_maxt, probs = 1-alpha, names = FALSE)
q_thr_stcs_maxt<- quantile(tmp_stcs_maxt, probs = 1-alpha, names = FALSE)
# retrieve false positives, i.e. values above current sample based threshold
fpr_stcs[i]<- tmp_stcs[length(tmp_stcs)] > q_thr_stcs
fpr_maxt[i]<- tmp_maxt[length(tmp_maxt)] > q_thr_maxt
# naive bivariate
# (either STCS or maxT is significant but on 0.025 alpha each)
fpr_bivariate[i]<- tmp_stcs[length(tmp_stcs)] > quantile(tmp_stcs, probs = 1-alpha/2, names = FALSE) | tmp_stcs_maxt[length(tmp_stcs_maxt)] > quantile(tmp_stcs_maxt, probs = 1-alpha/2, names = FALSE)
# slightly too liberal - mean and median =.055
# using bivariate empirical cdf
# library(bivariate)
# biv_distr<- ebvcdf(tmp_stcs, tmp_stcs_maxt)
# fpr_bivariate[i]<- biv_distr(tmp_stcs[length(tmp_stcs)], tmp_stcs_maxt[length(tmp_stcs_maxt)]) > (1-alpha)
# Too conservative
# Using empirical quantile contour line
# contour_line<- get_contour(tmp_stcs, tmp_stcs_maxt, alpha = alpha)
# fpr_bivariate[i]<- (tmp_stcs[length(tmp_stcs)] > max(quantile(tmp_stcs, probs = 1-alpha/2, names = FALSE), max(contour_line$x))) | (tmp_stcs_maxt[length(tmp_stcs_maxt)] > max(quantile(tmp_stcs_maxt, probs = 1-alpha/2, names = FALSE), max(contour_line$y)))
}
fpr_sim_stcs[j]<- sum(fpr_stcs)/length(fpr_stcs)
fpr_sim_maxt[j]<- sum(fpr_maxt)/length(fpr_maxt)
fpr_sim_bivariate[j]<- sum(fpr_bivariate)/length(fpr_bivariate)
}
par(mfrow = c(1, 3))
hist(fpr_sim_stcs)
hist(fpr_sim_maxt)
hist(fpr_sim_bivariate)
par(mfrow = c(1, 1))
summary(fpr_sim_stcs)
summary(fpr_sim_maxt)
summary(fpr_sim_bivariate)
fpr_sim_stcs<- vector(length = sim_length)
fpr_sim_maxt<- vector(length = sim_length)
fpr_sim_bivariate<- vector(length = sim_length)
for(j in 1:sim_length){
fpr_stcs<- vector(length = fpr_length)
fpr_maxt<- vector(length = fpr_length)
fpr_bivariate<- vector(length = fpr_length)
for (i in 1:fpr_length){
ind<- sample(x = length(perm_results$maxT), size = bootstrap_sample, replace = TRUE)
tmp_stcs<- perm_results$stcs[ind]
tmp_maxt<- perm_results$maxT[ind]
tmp_stcs_maxt<- perm_results$stcs_maxT[ind]
tmp_stcs_maxt[!is.finite(tmp_stcs_maxt)]<- 0
# gets the threshold for the current sample
q_thr_stcs<- quantile(tmp_stcs, probs = 1-alpha, names = FALSE)
q_thr_maxt<- quantile(tmp_maxt, probs = 1-alpha, names = FALSE)
q_thr_stcs_maxt<- quantile(tmp_stcs_maxt, probs = 1-alpha, names = FALSE)
# retrieve false positives, i.e. values above current sample based threshold
fpr_stcs[i]<- tmp_stcs[length(tmp_stcs)] > q_thr_stcs
fpr_maxt[i]<- tmp_maxt[length(tmp_maxt)] > q_thr_maxt
# naive bivariate
# (either STCS or maxT is significant but on 0.025 alpha each)
fpr_bivariate[i]<- tmp_stcs[length(tmp_stcs)] > quantile(tmp_stcs, probs = 1-alpha, names = FALSE) | tmp_stcs_maxt[length(tmp_stcs_maxt)] > quantile(tmp_stcs_maxt, probs = 1-alpha, names = FALSE)
# slightly too liberal - mean and median =.055
# using bivariate empirical cdf
# library(bivariate)
# biv_distr<- ebvcdf(tmp_stcs, tmp_stcs_maxt)
# fpr_bivariate[i]<- biv_distr(tmp_stcs[length(tmp_stcs)], tmp_stcs_maxt[length(tmp_stcs_maxt)]) > (1-alpha)
# Too conservative
# Using empirical quantile contour line
# contour_line<- get_contour(tmp_stcs, tmp_stcs_maxt, alpha = alpha)
# fpr_bivariate[i]<- (tmp_stcs[length(tmp_stcs)] > max(quantile(tmp_stcs, probs = 1-alpha/2, names = FALSE), max(contour_line$x))) | (tmp_stcs_maxt[length(tmp_stcs_maxt)] > max(quantile(tmp_stcs_maxt, probs = 1-alpha/2, names = FALSE), max(contour_line$y)))
}
fpr_sim_stcs[j]<- sum(fpr_stcs)/length(fpr_stcs)
fpr_sim_maxt[j]<- sum(fpr_maxt)/length(fpr_maxt)
fpr_sim_bivariate[j]<- sum(fpr_bivariate)/length(fpr_bivariate)
}
summary(fpr_sim_stcs)
summary(fpr_sim_maxt)
summary(fpr_sim_bivariate)
fpr_sim_stcs<- vector(length = sim_length)
fpr_sim_maxt<- vector(length = sim_length)
fpr_sim_bivariate<- vector(length = sim_length)
for(j in 1:sim_length){
fpr_stcs<- vector(length = fpr_length)
fpr_maxt<- vector(length = fpr_length)
fpr_bivariate<- vector(length = fpr_length)
for (i in 1:fpr_length){
ind<- sample(x = length(perm_results$maxT), size = bootstrap_sample, replace = TRUE)
tmp_stcs<- perm_results$stcs[ind]
tmp_maxt<- perm_results$maxT[ind]
tmp_stcs_maxt<- perm_results$stcs_maxT[ind]
tmp_stcs_maxt[!is.finite(tmp_stcs_maxt)]<- 0
# gets the threshold for the current sample
q_thr_stcs<- quantile(tmp_stcs, probs = 1-alpha, names = FALSE)
q_thr_maxt<- quantile(tmp_maxt, probs = 1-alpha, names = FALSE)
q_thr_stcs_maxt<- quantile(tmp_stcs_maxt, probs = 1-alpha, names = FALSE)
# retrieve false positives, i.e. values above current sample based threshold
fpr_stcs[i]<- tmp_stcs[length(tmp_stcs)] > q_thr_stcs
fpr_maxt[i]<- tmp_maxt[length(tmp_maxt)] > q_thr_maxt
# naive bivariate
# (either STCS or maxT is significant but on 0.025 alpha each)
fpr_bivariate[i]<- tmp_stcs[length(tmp_stcs)] > quantile(tmp_stcs, probs = 1-alpha/2, names = FALSE) | tmp_stcs_maxt[length(tmp_stcs_maxt)] > quantile(tmp_stcs_maxt, probs = 1-alpha/2, names = FALSE)
# slightly too liberal - mean and median =.055
# using bivariate empirical cdf
# library(bivariate)
# biv_distr<- ebvcdf(tmp_stcs, tmp_stcs_maxt)
# fpr_bivariate[i]<- biv_distr(tmp_stcs[length(tmp_stcs)], tmp_stcs_maxt[length(tmp_stcs_maxt)]) > (1-alpha)
# Too conservative
# Using empirical quantile contour line
# contour_line<- get_contour(tmp_stcs, tmp_stcs_maxt, alpha = alpha)
# fpr_bivariate[i]<- (tmp_stcs[length(tmp_stcs)] > max(quantile(tmp_stcs, probs = 1-alpha/2, names = FALSE), max(contour_line$x))) | (tmp_stcs_maxt[length(tmp_stcs_maxt)] > max(quantile(tmp_stcs_maxt, probs = 1-alpha/2, names = FALSE), max(contour_line$y)))
}
fpr_sim_stcs[j]<- sum(fpr_stcs)/length(fpr_stcs)
fpr_sim_maxt[j]<- sum(fpr_maxt)/length(fpr_maxt)
fpr_sim_bivariate[j]<- sum(fpr_bivariate)/length(fpr_bivariate)
}
fpr_sim_stcs<- vector(length = sim_length)
fpr_sim_maxt<- vector(length = sim_length)
fpr_sim_bivariate<- vector(length = sim_length)
for(j in 1:sim_length){
fpr_stcs<- vector(length = fpr_length)
fpr_maxt<- vector(length = fpr_length)
fpr_bivariate<- vector(length = fpr_length)
for (i in 1:fpr_length){
ind<- sample(x = length(perm_results$maxT), size = bootstrap_sample, replace = TRUE)
tmp_stcs<- perm_results$stcs[ind]
tmp_maxt<- perm_results$maxT[ind]
tmp_stcs_maxt<- perm_results$stcs_maxT[ind]
tmp_stcs_maxt[!is.finite(tmp_stcs_maxt)]<- 0
# gets the threshold for the current sample
q_thr_stcs<- quantile(tmp_stcs, probs = 1-alpha, names = FALSE)
q_thr_maxt<- quantile(tmp_maxt, probs = 1-alpha, names = FALSE)
q_thr_stcs_maxt<- quantile(tmp_stcs_maxt, probs = 1-alpha, names = FALSE)
# retrieve false positives, i.e. values above current sample based threshold
fpr_stcs[i]<- tmp_stcs[length(tmp_stcs)] > q_thr_stcs
fpr_maxt[i]<- tmp_maxt[length(tmp_maxt)] > q_thr_maxt
# naive bivariate
# (either STCS or maxT is significant but on 0.025 alpha each)
#fpr_bivariate[i]<- tmp_stcs[length(tmp_stcs)] > quantile(tmp_stcs, probs = 1-alpha/2, names = FALSE) | tmp_stcs_maxt[length(tmp_stcs_maxt)] > quantile(tmp_stcs_maxt, probs = 1-alpha/2, names = FALSE)
fpr_bivariate[i]<- tmp_stcs[length(tmp_stcs)] > quantile(tmp_stcs, probs = 1-alpha/2, names = FALSE) | tmp_maxt[length(tmp_maxt)] > quantile(tmp_maxt, probs = 1-alpha/2, names = FALSE)
# slightly too liberal - mean and median =.055
# using bivariate empirical cdf
# library(bivariate)
# biv_distr<- ebvcdf(tmp_stcs, tmp_stcs_maxt)
# fpr_bivariate[i]<- biv_distr(tmp_stcs[length(tmp_stcs)], tmp_stcs_maxt[length(tmp_stcs_maxt)]) > (1-alpha)
# Too conservative
# Using empirical quantile contour line
# contour_line<- get_contour(tmp_stcs, tmp_stcs_maxt, alpha = alpha)
# fpr_bivariate[i]<- (tmp_stcs[length(tmp_stcs)] > max(quantile(tmp_stcs, probs = 1-alpha/2, names = FALSE), max(contour_line$x))) | (tmp_stcs_maxt[length(tmp_stcs_maxt)] > max(quantile(tmp_stcs_maxt, probs = 1-alpha/2, names = FALSE), max(contour_line$y)))
}
fpr_sim_stcs[j]<- sum(fpr_stcs)/length(fpr_stcs)
fpr_sim_maxt[j]<- sum(fpr_maxt)/length(fpr_maxt)
fpr_sim_bivariate[j]<- sum(fpr_bivariate)/length(fpr_bivariate)
}
summary(fpr_sim_stcs)
summary(fpr_sim_maxt)
summary(fpr_sim_bivariate)
# go through the cluster properties / get_stcs step by step:
data=temp_gistemp
# detrend data ----
#***********************
sen0 <- function(y,x){
zyp.slopediff <- function(i, xx, yy, n) (yy[1:(n - i)] - yy[(i + 1):n])/(xx[1:(n - i)] - xx[(i + 1):n])
n <- length(y)
if (missing(x)) x <- c(1:n)
slopes <- unlist(lapply(1:(n - 1), zyp.slopediff, x, y, n))
return(median(slopes[is.finite(slopes)], na.rm=TRUE))
}
data_detrend<- data %>% apply(1:2, # apply(1:2,...) will apply function to every cell
function(x)
{
(x- 1:length(x)*sen0(x))
}
)
# go through the cluster properties / get_stcs step by step:
library(tidyverse)
data=temp_gistemp
# detrend data ----
#***********************
sen0 <- function(y,x){
zyp.slopediff <- function(i, xx, yy, n) (yy[1:(n - i)] - yy[(i + 1):n])/(xx[1:(n - i)] - xx[(i + 1):n])
n <- length(y)
if (missing(x)) x <- c(1:n)
slopes <- unlist(lapply(1:(n - 1), zyp.slopediff, x, y, n))
return(median(slopes[is.finite(slopes)], na.rm=TRUE))
}
data_detrend<- data %>% apply(1:2, # apply(1:2,...) will apply function to every cell
function(x)
{
(x- 1:length(x)*sen0(x))
}
)
data=temp_gistemp
# go through the cluster properties / get_stcs step by step:
library("devtools")
load_all()
data=temp_gistemp
# detrend data ----
#***********************
sen0 <- function(y,x){
zyp.slopediff <- function(i, xx, yy, n) (yy[1:(n - i)] - yy[(i + 1):n])/(xx[1:(n - i)] - xx[(i + 1):n])
n <- length(y)
if (missing(x)) x <- c(1:n)
slopes <- unlist(lapply(1:(n - 1), zyp.slopediff, x, y, n))
return(median(slopes[is.finite(slopes)], na.rm=TRUE))
}
data_detrend<- data %>% apply(1:2, # apply(1:2,...) will apply function to every cell
function(x)
{
(x- 1:length(x)*sen0(x))
}
)
data=temp_gistemp
# detrend data ----
#***********************
sen0 <- function(y,x){
zyp.slopediff <- function(i, xx, yy, n) (yy[1:(n - i)] - yy[(i + 1):n])/(xx[1:(n - i)] - xx[(i + 1):n])
n <- length(y)
if (missing(x)) x <- c(1:n)
slopes <- unlist(lapply(1:(n - 1), zyp.slopediff, x, y, n))
return(median(slopes[is.finite(slopes)], na.rm=TRUE))
}
data_detrend<- data %>% apply(1:2, # apply(1:2,...) will apply function to every cell
function(x)
{
(x- 1:length(x)*sen0(x))
}
)
data_detrend <-  aperm(data_detrend, c(2,3,1)) # transpose it to put lat & long in the first dimensions again
# set options ----
#***********************
fx=sample_mk_function
method="all"
nperm=20
alpha_local=0.05
alpha_global=0.05
nperm=4
alpha_local=0.05
alpha_global=0.05
null_distribution="normal" # defines if threshold based on alpha level is drawn from normal or t distribution
seed=NULL
block_size=NULL
verbose=TRUE
data=data_detrend
# perm_dist ----
#***********************
perm_matrix<- perm_matrix(nobs = dim(data)[3], nperm = nperm, block_size = block_size, seed = seed)
maxT<- vector(length = nperm)
stcs<- vector(length = nperm)
stcs_maxT_all<- vector(length = nperm)
# get_stcs step by step ----
#***********************
tmp<- apply(data[,,perm_matrix[3,]], 1:2, fx)
data = tmp
if(null_distribution == "normal") thr<- qnorm(1-alpha_local/2)
if(null_distribution == "t") thr<- qt(1-alpha_local/2, df = data_dim[3]-2)
pixel_sign<- sign(data)
pixel_significant<- abs(data)>thr
pixel_result<- pixel_sign*pixel_significant
# positive
pixel_result_pos<- pixel_result
pixel_result_pos[is.na(pixel_result_pos)]<- -999
clusters_pos<- osc::cca(pixel_result_pos,count.cells = TRUE, s=1, mode = 2, # only values >0 are included in osc:cca
count.max  = length(pixel_sign))
stcs_pos<- max(clusters_pos$cluster.count)
nclust_pos<- length(clusters_pos$cluster.count)
clusters_sep<- vector(mode = "list", length = 2)
# negative
pixel_result_neg<- pixel_result
pixel_result_neg[pixel_result_neg == -1] = 10 # swap signs so that originally negative values will now be considered in osc:cca
pixel_result_neg[pixel_result_neg == 1] = -10
pixel_result_neg[is.na(pixel_result_neg)] = -999
clusters_neg<- osc::cca(pixel_result_neg,count.cells = TRUE, s=1, mode = 2,
count.max = length(pixel_sign))
stcs_neg<- max(clusters_neg$cluster.count)
# join
clusters_neg$clusters[clusters_neg$clusters > 0]<- clusters_neg$clusters[clusters_neg$clusters > 0] + nclust_pos
clusters_sep[[1]]<- clusters_pos$clusters + clusters_neg$clusters
clusters_sep[[2]]<- c(clusters_pos$cluster.count, clusters_neg$cluster.count)
names(clusters_sep)<- c("clusters", "cluster.count")
View(clusters_sep)
clusters_sep
clusters_sep$cluster.count
unique(clusters_sep$clusters)
unique(c(clusters_sep$clusters))
dim(data)
sum(clusters_sep$cluster.count)
allcluster_max <- c()
for (i in 1:length(clusters_sep$cluster.count)){ # retrieve maximum of each cluster
clust_max <- data[clusters_sep$clusters==i] %>%
max(.,na.rm = TRUE) %>%
cluster_sep$cluster.max <- clust_max # assign each cluster its maximum
allcluster_max <- c(allcluster_max, clust_max)
}
allcluster_max <- c()
for (i in 1:length(clusters_sep$cluster.count)){ # retrieve maximum of each cluster
clust_max <- data[clusters_sep$clusters==i] %>%
max(.,na.rm = TRUE)
cluster_sep$cluster.max <- clust_max # assign each cluster its maximum
allcluster_max <- c(allcluster_max, clust_max)
}
for (i in 1:length(clusters_sep$cluster.count)){ # retrieve maximum of each cluster
clust_max <- data[clusters_sep$clusters==i] %>%
max(.,na.rm = TRUE)
clusters_sep$cluster.max <- clust_max # assign each cluster its maximum
allcluster_max <- c(allcluster_max, clust_max)
}
stcs_maxT_all <- max(allcluster_max, na.rm = TRUE)
stcs<- max(clusters_sep$cluster.count, na.rm = TRUE)
View(clusters_sep)
View(clusters_sep)
clusters_sep$cluster.max
allcluster_max <- c()
clusters_sep$cluster.max <- vector(mode = "list", length = length(clusters_pos$cluster.count))
for (i in 1:length(clusters_sep$cluster.count)){ # retrieve maximum of each cluster
clust_max <- data[clusters_sep$clusters==i] %>%
max(.,na.rm = TRUE)
clusters_sep$cluster.max <- clust_max # assign each cluster its maximum
allcluster_max <- c(allcluster_max, clust_max)
}
stcs_maxT_all <- max(allcluster_max, na.rm = TRUE)
View(clusters_sep)
clusters_sep$cluster.max <- vector(mode = "list", length = length(clusters_pos$cluster.count))
View(clusters_sep)
for (i in 1:length(clusters_sep$cluster.count)){ # retrieve maximum of each cluster
clust_max <- data[clusters_sep$clusters==i] %>%
max(.,na.rm = TRUE)
clusters_sep$cluster.max[i] <- clust_max # assign each cluster its maximum
allcluster_max <- c(allcluster_max, clust_max)
}
stcs_maxT_all <- max(allcluster_max, na.rm = TRUE)
View(clusters_sep)
clusters_sep$cluster.max <- vector(length = length(clusters_pos$cluster.count))
for (i in 1:length(clusters_sep$cluster.count)){ # retrieve maximum of each cluster
clust_max <- data[clusters_sep$clusters==i] %>%
max(.,na.rm = TRUE)
clusters_sep$cluster.max[i] <- clust_max # assign each cluster its maximum
allcluster_max <- c(allcluster_max, clust_max)
}
stcs_maxT_all <- max(allcluster_max, na.rm = TRUE)
View(clusters_sep)
